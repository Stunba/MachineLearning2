{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная работа №7. Рекуррентные нейронные сети для анализа текста\n",
    "=====\n",
    "Данные: Набор данных для предсказания оценок для отзывов, собранных с сайта imdb.com, который состоит из 50,000 отзывов в виде текстовых файлов. Отзывы разделены на положительные (25,000) и отрицательные (25,000). Данные предварительно токенизированы по принципу “мешка слов”, индексы слов можно взять из словаря (imdb.vocab). Обучающая выборка включает в себя 12,500 положительных и 12,500 отрицательных отзывов, контрольная выборка также содержит 12,500 положительных и 12,500 отрицательных отзывов, а также. Данные можно скачать по ссылке https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузите данные. Преобразуйте текстовые файлы во внутренние структуры данных, которые используют индексы вместо слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 33.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Clint', 'Eastwood', 'would', 'star', 'again', 'as', 'the', 'battle', '-', 'weary', 'Detective', 'Harry', 'Callahan', ',', 'but', 'would', 'also', 'direct', 'the', 'fourth', 'entry', 'in', 'the', \"'\", 'Dirty', 'Harry', \"'\", 'series', '.', \"'\", 'Sudden', 'Impact', \"'\", 'again', 'like', 'the', 'other', 'additions', ',', 'brings', 'its', 'own', 'distinguishable', 'style', 'and', 'tone', ',', 'but', 'if', 'anything', 'it', \"'s\", 'probably', 'the', 'most', 'similar', 'to', 'the', 'original', 'in', 'it', \"'s\", 'darker', 'and', 'seedy', 'moments', '(', 'and', 'bestowing', 'a', 'classic', 'line', '\"', 'Go', 'ahead', '.', 'Make', 'my', 'day', '\"', ')', '\\x85 ', 'but', 'some', 'of', 'its', 'humor', 'has', 'to', 'been', 'seen', 'to', 'believe', '.', 'A', 'bulldog', '\\x85 ', 'named', 'meathead', 'that', 'pisses', 'and', 'farts', '.', 'Oh', 'yeah', '.', 'However', 'an', 'interesting', 'fact', 'this', 'entry', 'was', 'only', 'one', 'in', 'series', 'to', 'not', 'have', 'it', 'set', 'entirely', 'in', 'San', 'Francisco.<br', '/><br', '/>The', 'story', 'follows', 'that', 'of', 'detective', 'Callahan', 'trying', 'to', 'put', 'the', 'pieces', 'together', 'of', 'a', 'murder', 'where', 'the', 'victim', 'was', 'shot', 'in', 'the', 'groin', 'and', 'then', 'between', 'the', 'eyes', '.', 'After', 'getting', 'in', 'some', 'trouble', 'with', 'office', 'superiors', 'and', 'causing', 'a', 'stir', 'which', 'has', 'some', 'crime', 'lord', 'thugs', 'after', 'his', 'blood', '.', 'He', \"'s\", 'ordered', 'to', 'take', 'leave', ',', 'but', 'it', 'falls', 'into', 'a', 'working', 'one', 'where', 'he', 'heads', 'to', 'a', 'coastal', 'town', 'San', 'Paulo', ',', 'where', 'a', 'murder', 'has', 'occurred', 'similar', 'in', 'vein', '(', 'bullet', 'to', 'groin', 'and', 'between', 'eyes', ')', 'to', 'his', 'case', '.', 'There', 'he', 'begins', 'to', 'dig', 'up', 'dirt', ',', 'which', 'leads', 'to', 'the', 'idea', 'of', 'someone', 'looking', 'for', 'revenge.<br', '/><br', '/>To', 'be', 'honest', ',', 'I', 'was', \"n't\", 'all', 'that', 'crash', 'hot', 'on', 'Eastwood', \"'s\", 'take', ',', 'but', 'after', 'many', 'repeat', 'viewings', 'it', 'virtually', 'has', 'grown', 'on', 'me', 'to', 'the', 'point', 'of', 'probably', 'being', 'on', 'par', 'with', 'the', 'first', 'sequel', \"'\", 'Magnum', 'Force', \"'\", '.', 'This', 'well', '-', 'assembled', 'plot', 'actually', 'gives', 'Eastwood', 'another', 'angle', 'to', 'work', 'upon', '(', 'even', 'though', 'it', 'feels', 'more', 'like', 'a', 'sophisticated', 'take', 'on', 'the', 'vigilante', 'features', 'running', 'rampant', 'at', 'that', 'time', ')', ',', 'quite', 'literal', 'with', 'something', 'punishing', 'but', 'luridly', 'damaging', '.', 'It', \"'s\", 'like', 'he', \"'s\", 'experimenting', 'with', 'noir', '-', 'thriller', 'touches', 'with', 'character', '-', 'driven', 'traits', 'to', 'help', 'develop', 'the', 'emotionally', 'bubbling', 'and', 'eventual', 'morality', 'framework', '.', 'His', 'use', 'of', 'images', 'is', 'lasting', ',', 'due', 'to', 'its', 'slickly', 'foreboding', 'atmospherics', '.', 'Dark', 'tones', ',', 'brooding', 'lighting', '\\x85 ', 'like', 'the', 'scene', 'towards', 'the', 'end', 'akin', 'to', 'some', 'western', 'showdown', 'of', 'a', 'silhouette', 'figure', '(', 'Harry', 'with', 'his', 'new', '.44', 'automag', 'handgun', ')', 'moving', 'its', 'way', 'towards', 'the', 'stunned', 'prey', 'on', 'the', 'fishing', 'docks', '.', 'It', \"'s\", 'a', 'striking', 'sight', 'that', 'builds', 'fear', '!', 'Mixing', 'the', 'hauntingly', 'cold', 'with', 'plain', 'brutality', 'and', 'dash', 'of', 'humor', '.', 'It', 'seemed', 'to', 'come', 'off', '.', 'A', 'major', 'plus', 'with', 'these', 'films', 'are', 'the', 'dialogues', ',', 'while', 'I', 'would', \"n't\", 'call', \"'\", 'Sudden', 'Impact', \"'\", 'first', '-', 'rate', ',', 'it', 'provides', 'ample', 'biting', 'exchanges', 'and', 'memorably', 'creditable', 'lines', '\\x85 ', '\"', 'You', \"'re\", 'a', 'legend', 'in', 'your', 'own', 'mind', '\"', '.', 'Do', \"n't\", 'you', 'just', 'love', 'hearing', 'Harry', 'sparking', 'an', 'amusing', 'quip', ',', 'before', 'pulling', 'out', 'his', 'piece', '.', 'The', 'beating', 'action', 'when', 'it', 'occurs', 'is', 'excitingly', 'jarring', 'and', 'intense', '\\x85 ', 'the', 'only', 'way', 'to', 'go', 'and', 'the', 'pacing', 'flies', 'by', 'with', 'little', 'in', 'the', 'way', 'of', 'flat', 'passages', '.', 'Lalo', 'Schfrin', 'would', 'return', 'as', 'composer', '(', 'after', \"'\", 'The', 'Enforcer', '\"', 'had', 'Jerry', 'Fielding', 'scoring', ')', 'bringing', 'a', 'methodical', 'funky', 'kick', ',', 'which', 'still', 'breathed', 'those', 'gloomy', 'cues', 'to', 'a', 'texturally', 'breezy', 'score', 'that', 'clicked', 'from', 'the', 'get', '-', 'go', '.', 'Bruce', 'Surtees', '(', 'an', 'Eastwood', 'regular', ')', 'gets', 'the', 'job', 'behind', 'the', 'camera', '(', 'where', 'he', 'did', 'a', 'piecing', 'job', 'with', \"'\", 'Dirty', 'Harry', \"'\", ')', 'and', 'gives', 'the', 'film', 'plenty', 'of', 'scope', 'by', 'wonderfully', 'framing', 'the', 'backdrops', 'in', 'some', 'impeccable', 'tracking', 'scenes', ',', 'but', 'also', 'instrument', 'edgy', 'angles', 'within', 'those', 'dramatic', 'moments.<br', '/><br', '/>Eastwood', 'as', 'the', 'dinosaur', 'Callahan', 'still', 'packs', 'a', 'punch', ',', 'going', 'beyond', 'just', 'that', 'steely', 'glare', 'to', 'get', 'the', 'job', 'done', 'and', 'probably', 'showing', 'a', 'little', 'more', 'heart', 'than', 'one', 'would', 'expect', 'from', 'a', 'younger', 'Callahan', '.', 'This', 'going', 'by', 'the', 'sudden', 'shift', 'in', 'a', 'plot', 'turn', 'of', 'Harry', \"'s\", 'quest', 'for', 'justice', '\\x85 ', 'by', 'the', 'badge', 'even', 'though', 'he', 'does', \"n't\", 'always', 'agree', 'with', 'it', '.', 'I', 'just', 'found', 'it', 'odd', '\\x85 ', 'a', 'real', 'change', 'of', 'heart', '.', 'Across', 'from', 'him', 'is', 'a', 'stupendous', 'performance', 'by', 'his', 'beau', 'at', 'the', 'time', 'Sondra', 'Locke', '.', 'Her', 'turn', 'of', 'traumatic', 'torment', '(', 'being', 'senselessly', 'raped', 'along', 'with', 'her', 'younger', 'sister', ')', ',', 'is', 'hidden', 'by', 'a', 'glassily', 'quiet', 'intensity', '.', 'When', 'the', 'anger', 'is', 'released', ',', 'it', \"'s\", 'tactically', 'accurate', 'in', 'its', 'outcome', '.', 'Paul', 'Drake', 'is', 'perfectly', 'menacing', 'and', 'filthy', 'as', 'one', 'of', 'the', 'targeted', 'thugs', 'and', 'Audrie', 'J.', 'Neenan', 'nails', 'down', 'a', 'repellently', 'scummy', 'and', 'big', '-', 'mouthed', 'performance', '.', 'These', 'people', 'are', 'truly', 'an', 'ugly', 'bunch', 'of', 'saps', '.', 'Pat', 'Hingle', 'is', 'sturdy', 'as', 'the', 'Chief', 'of', 'the', 'small', 'coastal', 'town', '.', 'In', 'smaller', 'parts', 'are', 'Bradford', 'Dillman', 'and', 'the', 'agreeably', 'potent', 'Albert', 'Popwell', '(', 'a', 'regular', 'in', 'the', 'series', '1', '-', '4', ',', 'but', 'under', 'different', 'characters', ')', '.', 'How', 'can', 'you', 'forget', 'him', 'in', \"'\", 'Dirty', 'Harry', \"'\", '\\x85 ', 'yes', 'he', 'is', 'bank', 'robber', 'that', \"'s\", 'at', 'the', 'end', 'of', 'the', 'trademark', 'quote', '\"', 'Do', 'I', 'feel', 'lucky', '?', 'Well', ',', 'do', 'ya', ',', 'punk', '?', '\"'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = vocab_size)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 204019),\n",
       " (',', 193654),\n",
       " ('.', 166331),\n",
       " ('and', 110329),\n",
       " ('a', 110150),\n",
       " ('of', 101409),\n",
       " ('to', 94242),\n",
       " ('is', 76799),\n",
       " ('in', 61400),\n",
       " ('I', 54426),\n",
       " ('it', 54053),\n",
       " ('that', 49499),\n",
       " ('\"', 44109),\n",
       " (\"'s\", 43703),\n",
       " ('this', 42479),\n",
       " ('-', 37154),\n",
       " ('/><br', 36011),\n",
       " ('was', 35256),\n",
       " ('as', 30408),\n",
       " ('with', 30268)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализуйте и обучите двунаправленную рекуррентную сеть (LSTM или GRU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device = 'cuda'\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = batch_size,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(TEXT.vocab)\n",
    "embedding_size = 100\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import RNN\n",
    "\n",
    "model = RNN(input_size, embedding_size, hidden_size, output_size, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "logs_writer = SummaryWriter(log_dir='./logs/rnn')\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:03<00:00, 132.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "\n",
    "train(model, train_iterator, valid_iterator, optimizer, criterion, logs_writer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.482 | Test Acc: 77.93%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используйте индексы слов и их различное внутреннее представление (word2vec, glove)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "# build the vocabulary\n",
    "embedding_size = 100\n",
    "TEXT.build_vocab(train_data, max_size = vocab_size, vectors=GloVe(name='6B', dim=embedding_size))\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, embedding_size, hidden_size, output_size, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_size)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_size)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device = 'cuda'\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = batch_size,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(TEXT.vocab)\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "logs_writer = SummaryWriter(log_dir='./logs/glove')\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 1/5 [02:12<08:48, 132.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 2/5 [04:25<06:37, 132.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 3/5 [06:37<04:24, 132.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 4/5 [08:51<02:12, 132.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 5/5 [11:06<00:00, 133.23s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion, logs_writer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.356 | Test Acc: 87.19%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поэкспериментируйте со структурой сети (добавьте больше рекуррентных, полносвязных или сверточных слоев)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import RNN2\n",
    "\n",
    "model = RNN2(input_size, embedding_size, hidden_size, output_size, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_size)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "logs_writer = SummaryWriter(log_dir='./logs/rnn-additional_fc')\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 1/5 [02:13<08:54, 133.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [04:27<06:40, 133.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [06:40<04:26, 133.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [08:54<02:13, 133.75s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [11:08<00:00, 133.78s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion, logs_writer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.407 | Test Acc: 84.31%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/Stunba/MachineLearning2/blob/master/lab7/Screenshot%202020-04-18%20at%2019.42.03.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "В данной работе была реализованная рекуррентная сеть для анализа текста, были использованные разные модели сети и способы векторизации текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
